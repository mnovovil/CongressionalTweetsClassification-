{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kaggle Competition ##  \n",
        "Lisa Pink and Miguel Novo Villar - DSCC465: Introduction to Statistical Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj53C0px08Z9",
        "outputId": "0dfd2fa1-40e6-4b35-da29-9076982487e4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt    \n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu951MUL14oi"
      },
      "outputs": [],
      "source": [
        "np.random.seed(265)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytE_KkbHNFFF"
      },
      "source": [
        "Options to improve performace\n",
        "1. Try other models\n",
        "2. Try other vectorizers\n",
        "3. See the balance of the data set (num of D vs num of R)\n",
        "4. Test select data with https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html --> Stratifyv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2KgAisV08aD"
      },
      "source": [
        "## DATA CLEANING ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgXHs7EM08aF"
      },
      "outputs": [],
      "source": [
        "#Cleaning the text. \n",
        "# Source: https://www.kaggle.com/code/clmentbisaillon/twitter-customer-support-data-cleaning\n",
        "rare = re.compile(r\"\\^\\S*\")\n",
        "new_line = re.compile(r\"\\n+\\S*\")\n",
        "sig = re.compile(r\"-\\S*\")\n",
        "\n",
        "#Initial preprocessing function\n",
        "def preprocessor(data):\n",
        "    corpus = []\n",
        "    for i in range(len(data)):\n",
        "        #remove urls\n",
        "        tweet= re.sub(r'http\\S+', ' ', data[\"full_text\"][i][2:-1]) #[2:-1] removes the b' caracters\n",
        "\n",
        "        #remove mentions\n",
        "        tweet = re.sub('@[A-Za-z0–9]+', '', tweet)\n",
        "        tweet = re.sub(\"@[\\w]*\",\"\",tweet)\n",
        "        \n",
        "        # # Contractions (Source: https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook)\n",
        "        tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "        tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "        tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "        tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "        tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "        tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "        tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "        tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "        tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "        tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "        tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "        tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "        tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "        tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "        tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "        tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "        tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "        tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "        tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "        tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "        tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "        tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "        tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "        tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "        tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "        tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "        tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "        tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "        tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "        tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "        tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "        tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "        tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "        tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "        tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "        tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "        tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "        tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "        tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "        tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "        tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "        tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "        tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "        tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "        tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "        tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "        tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "        tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "        tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "        tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "        tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "        tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "        tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "        tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "        tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "        tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "        tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "        tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "        tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "        tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "        tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "        tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "        tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "        tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "        tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "        tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "        tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "        tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "        tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "        tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "        tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "        tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "        tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "        tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "        tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "        tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "        tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "        tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "        tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "        tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "        tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "        tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "        tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "        tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "        tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "        tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "        tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "        tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "        tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "        tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "        tweet = re.sub(r\"donå«t\", \"do not\", tweet) \n",
        "        \n",
        "        # Character entity references\n",
        "        tweet = re.sub(r\"&gt;\", \"\", tweet)\n",
        "        tweet = re.sub(r\"&lt;\", \"\", tweet)\n",
        "        tweet = re.sub(r\"&amp;\", \"\", tweet)\n",
        "\n",
        "        # Typos, slang and informal abbreviations\n",
        "        tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "        tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "        tweet = re.sub(r\"USAgov\", \"USA government\", tweet)        \n",
        "\n",
        "        #remove rt\n",
        "        tweet = re.sub(\"RT @[\\w]*:\",\"\",tweet)#removing rt\n",
        "        tweet = re.sub('RT[\\s]+', '', tweet) # Removing RT\n",
        "        \n",
        "        #remove emoji\n",
        "        tweet=re.sub(\"[^\\w\\s#@/:%.,_-]\", \"\", tweet, flags=re.UNICODE)#remove emoji\n",
        "        \n",
        "        tweet = tweet.replace('x', '')\n",
        "        \n",
        "        #remove html tags\n",
        "        tweet = re.sub(r'<.*?>',' ', tweet) \n",
        "\n",
        "        #signatures\n",
        "        tweet = sig.sub(r'', tweet)\n",
        "\n",
        "        #rare\n",
        "        tweet = rare.sub(r'', tweet)\n",
        "\n",
        "        #new line\n",
        "        tweet = new_line.sub(r'.', tweet)\n",
        "\n",
        "        #remove digits\n",
        "        tweet = re.sub(r'\\d+',' ', tweet)\n",
        "        \n",
        "        # #remove hashtags\n",
        "        # tweet = re.sub(r'#\\w+',' ', tweet)\n",
        "        \n",
        "        #remove white\n",
        "        tweet = re.sub(\"^\\\\s+|\\\\s+$\", \"\", tweet)  # Remove leading and trailing white space\n",
        "        #unite multispace\n",
        "        tweet = ' '.join(tweet.split())\n",
        "        review = re.sub('[^a-zA-Z]', ' ', tweet)\n",
        "\n",
        "        review = review.lower()\n",
        "        review = review.split()\n",
        "\n",
        "        review = ' '.join(review)\n",
        "        corpus.append(review)\n",
        "    return corpus   \n",
        "\n",
        "#FUNCTIONS APPLIED\n",
        "\n",
        "def convert_list_of_str_to_list_lists(X):\n",
        "    return [list(sentence) for sentence in X]\n",
        "\n",
        "def remove_stopwords(word_tokens):\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend([\"amp\",\"wa\",\"ta\",\"ha\",\"nn\",\"ie\",\"ste\"])\n",
        "    return [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "def tokenize_words(X, tweet_tokenizer=True, rmv_stopwords=True):\n",
        "    if tweet_tokenizer:\n",
        "        tokenize = TweetTokenizer().tokenize\n",
        "    else:\n",
        "        tokenize = nltk.word_tokenize\n",
        "\n",
        "    word_tokens = [tokenize(sentence) for sentence in X]\n",
        "\n",
        "    if rmv_stopwords:\n",
        "        return [remove_stopwords(tokens) for tokens in word_tokens]\n",
        "    else:\n",
        "        return word_tokens\n",
        "\n",
        "def pos_tagging_words(X):\n",
        "    return [[nltk.pos_tag(sentence)] for sentence in X]\n",
        "\n",
        "def lemmatize_words(X, lemmatizer):\n",
        "    X_lemmatize = list()\n",
        "    for sentence in X:\n",
        "        X_lemmatize.append([[lemmatizer.lemmatize(word)] for word in sentence])\n",
        "    return X_lemmatize\n",
        "\n",
        "def prepare_tokens_for_vectorize(X):\n",
        "    X_preproc = list()\n",
        "    for sentence_tok in X:\n",
        "        tokens = [tok[0] for tok in sentence_tok]\n",
        "        X_preproc.append(\" \".join(tokens))\n",
        "\n",
        "    return X_preproc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEU_TbLD7RQk"
      },
      "outputs": [],
      "source": [
        "training_df = pd.read_csv('congressional_tweet_training_data.csv')\n",
        "test_df = pd.read_csv('congressional_tweet_test_data.csv')\n",
        "\n",
        "X_train = preprocessor(training_df)\n",
        "y_train = np.array(training_df[\"party\"].eq('D').mul(1))\n",
        "\n",
        "X_test = preprocessor(test_df)\n",
        "y_test = np.array(test_df[\"party\"].eq('D').mul(1))\n",
        "\n",
        "X_token_train = tokenize_words(X_train)\n",
        "X_token_test = tokenize_words(X_test)\n",
        "\n",
        "# X_pos_tag_train = pos_tagging_words(X_token_train)\n",
        "# X_pos_tag_test = pos_tagging_words(X_token_test)\n",
        "\n",
        "X_lemma_train = lemmatize_words(X_token_train, WordNetLemmatizer())\n",
        "X_lemma_test = lemmatize_words(X_token_test, WordNetLemmatizer())\n",
        "\n",
        "X_prep_train = prepare_tokens_for_vectorize(X_lemma_train)\n",
        "X_prep_test = prepare_tokens_for_vectorize(X_lemma_test)\n",
        "\n",
        "#Not using\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=False)\n",
        "# vectorizer.fit(X_prep_train+X_prep_test)\n",
        "\n",
        "# X_final_train = vectorizer.transform(X_prep_train)\n",
        "# X_final_test = vectorizer.transform(X_prep_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model used to wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "train_republican = train_clean.loc[(train_clean[\"party\"] == \"R\")] #republican slice\n",
        "train_democrat = train_clean.loc[(train_clean[\"party\"] == \"D\")] #republican slice\n",
        "\n",
        "train_republican = [' '.join(train_republican['text_clean_train'])]\n",
        "train_democrat = [' '.join(train_democrat['text_clean_train'])]\n",
        "\n",
        "\n",
        "training_df.head(2)\n",
        "from collections import Counter\n",
        "Counter(\" \".join(training_df[\"hashtags\"]).split()).most_common(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODELS ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of the processing function that goes into the vectorizer is X_prep_train and X_prep_test. This is a very clean version of our data. At this point we just need to play with the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_RD = {0: \"R\", 1: \"D\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer5 = TfidfVectorizer(ngram_range=(1,2), lowercase= True)\n",
        "#Train\n",
        "train_x5 = vectorizer5.fit_transform(X_prep_train)\n",
        "#Test\n",
        "test_x5 = vectorizer5.transform(X_prep_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selected model among all is Logistic Regression, several other models were approached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "o9nblZgCI04V",
        "outputId": "8cc66093-a26f-4d64-ecc3-c7b4dc9f4ea1"
      },
      "outputs": [],
      "source": [
        "log_regression = LogisticRegression(max_iter=10000, n_jobs = -1, solver = \"saga\").fit(X_final_train, y_train)\n",
        "log_regression_pred = log_regression.predict(X_final_test)\n",
        "prediction_model1 = pd.DataFrame(log_regression_pred, columns=['predictions_model1'])\n",
        "prediction_model1['predictions_model1'] = prediction_model1['predictions_model1'].map(transform_RD).to_csv('prediction_model_model1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other potential models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier \n",
        "mlp=MLPClassifier(random_state=265, early_stopping=True, verbose=2)\n",
        "mlp.fit(train_x5, y)\n",
        "mlp_pred = mlp.predict(test_x5) \n",
        "#Possible models\n",
        "\n",
        "model2 = MLPClassifier(random_state=265, activation='identity', max_iter=10000).fit(train_x5, y)\n",
        "model2_pred = model2.predict(test_x5) \n",
        "\n",
        "model3 = MLPClassifier(random_state=265, activation='logistic', max_iter=10000).fit(train_x5, y)\n",
        "model3_pred = model3.predict(test_x5)\n",
        "\n",
        "model4 = MLPClassifier(random_state=265, activation='tanh', max_iter=10000).fit(train_x5, y)\n",
        "model4_pred = model4.predict(test_x5) \n",
        "\n",
        "model5 = MLPClassifier(random_state=265, activation='relu', max_iter=10000).fit(train_x5, y)\n",
        "model5_pred = model5.predict(test_x5) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Potential submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_model1 = pd.DataFrame(model1_pred, columns=['predictions_model1']).to_csv('prediction_model1.csv')\n",
        "prediction_model2 = pd.DataFrame(model2_pred, columns=['predictions_model2']).to_csv('prediction_model2.csv')\n",
        "prediction_model3 = pd.DataFrame(model3_pred, columns=['predictions_model3']).to_csv('prediction_model3.csv')\n",
        "prediction_model4 = pd.DataFrame(model4_pred, columns=['predictions_model4']).to_csv('prediction_model4.csv')\n",
        "prediction_model5 = pd.DataFrame(model5_pred, columns=['predictions_model5']).to_csv('prediction_model5.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Kaggle Competition1.2_working.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "50cad90f2f273938ceabd83189106b19f0fe603f9ecc8bddf15d51c3f9654c30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
